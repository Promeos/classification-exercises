{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model_evaluation_functions import model_container, models, outcome_matrix, evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Artisanal Evaluation\n",
    "Given the following confusion matrix, evaluate (_by hand_) the model's performance.\n",
    "\n",
    "|               | actual cat | actual dog |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| predicted cat |         34 |          7 |\n",
    "| predicted dog |         13 |         46 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artisanal Evaluation\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "print(\"Artisanal Evaluation\")\n",
    "# Image(filename='./evaluations-by-hand.png', width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context:\n",
    "\n",
    "    On earth 617, the only animals are cats and dogs. Cats and dogs roam freely on the earth. Certain human communities can only be around dogs, because they're allergic to cats. Cats must be kept away from these communities at all costs, or they will sneeze louder than 12 Saturn V Rockets. \n",
    "\n",
    "    They've recruited Chris and the gang from good ol' earth 616 to help them evaluate their defense model. The model predicts whether an animal is a cat or not a cat. If the model predicts a cat, industrial-sized presentation lasers will point at the ground and lead the cats away from the community. Dogs ignore the lasers and mark their territory. If a cat enters the community, well, RIP ear drums.\n",
    "\n",
    "In the context of this problem, what is a false positive (Type I Error)?\n",
    "> In the context of is this problem, a False Positive means that the model __predicted a picture to be a Cat__, but it was __actually a Dog__.\n",
    "- _Close call_\n",
    "\n",
    "In the context of this problem, what is a false negative (Type II Error)?\n",
    "> In the context of is this problem, a False Negative means that the model __predicted a picture to be a Dog__, but it was __actually a Cat__.\n",
    "- _RIP eardrums_\n",
    "\n",
    "How would you describe this model?\n",
    "> This model predicts whether an animal is a cat or not a cat.\n",
    "- \"Not a cat\" == Dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. C3 Rubber Duck Manufacturer\n",
    "You are working as a data scientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects.\n",
    "Use the predictions dataset and pandas to help answer the following questions:\n",
    "\n",
    "An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load c3's baseline model data - 3 models\n",
    "df_c3 = pd.read_csv('./c3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c3['baseline'] = df_c3.actual.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3   baseline\n",
       "0  No Defect  No Defect  Defect  No Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect  No Defect\n",
       "2  No Defect  No Defect  Defect  No Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect  No Defect\n",
       "4  No Defect  No Defect  Defect  No Defect  No Defect"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Series name</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Row ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>16.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>8</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Series name           Row ID          \n",
       "actual         Defect No Defect Defect No Defect\n",
       "No Defect        16.0     184.0      8       182\n",
       "Defect            NaN       NaN      8         2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create confusion matrices for each model to evaluate their performance.\n",
    "baseline_outcome_matrix = pd.crosstab(df_c3.baseline, df_c3.actual)\n",
    "model1_outcome_matrix = pd.crosstab(df_c3.model1, df_c3.actual)  # model 1\n",
    "model2_outcome_matrix = pd.crosstab(df_c3.model2, df_c3.actual)  # model 2\n",
    "model3_outcome_matrix = pd.crosstab(df_c3.model3, df_c3.actual)  # model 3\n",
    "pd.concat([baseline_outcome_matrix, model1_outcome_matrix], axis=1, keys=['Series name', 'Row ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate each model's performance\n",
    "\n",
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Specificity 92.00%\n"
     ]
    }
   ],
   "source": [
    "false_n, true_n = baseline_outcome_matrix.values.ravel()\n",
    "\n",
    "basemodel_specificity = true_n / (true_n + false_n)\n",
    "print(f\"Baseline Specificity {basemodel_specificity:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_1_evaluation = evaluation_metrics(model1_outcome_matrix, model_number='1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_evaluation = evaluation_metrics(model2_outcome_matrix, model_number='2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_evaluation = evaluation_metrics(model3_outcome_matrix, model_number='3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Model Metrics\n",
    "pd.concat("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which evaluation metric would be appropriate here?\n",
    "\n",
    ">__Answer__: The most appropriate metric for C3's business problem is **Recall**.\n",
    "The model identifies defective or non-defective rubber ducks. If the model predicts \"no defect\" when there is a \"defect\" (Type II Error), our customer receives a defective rubber ducky.\n",
    "\n",
    ">__Reasoning__: Evaluating ducks classified as \"no-defect\" (True Positives and False Negatives) helps us evaluate the model's performance.\n",
    "- If the recall rate is high, the model can determine what a __True__ non-defective rubber duck is.\n",
    "- If the recall rate is low, the model is sending out truckloads of defective rubber ducks.\n",
    "\n",
    "\n",
    "2. Which model would be the best fit for this use case?\n",
    "> Model 3 would be the best fit for this use case because it has the __highest__ _recall_.\n",
    "\n",
    "Model 3 Evaluation\n",
    "- Accuracy               55.50%\n",
    "- Recall                 81.25%\n",
    "- Precision              13.13%\n",
    "- Specificity            53.26%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "        Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii.\n",
    "> DANG. Talk about upholding their reputation. They've put the pressure on... they've hired one of the best in the business, `IGOT this`. I need to minimize those expensive vacations.\n",
    "\n",
    "        They need you to predict which ducks will have defects, but tell you they really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here?\n",
    "> __Answer__: *__Precision__* is the appropriate metric to evaluate the models.\n",
    "\n",
    ">__Reasoning__: Evaluating ducks classified as \"defect\" (True Positives and False Positives) helps us evaluate the model's performance. Due to C3's PR stunt, we also need to make sure that people don't get a defective free duck AND get a vacation to Hawaii.\n",
    "- Note: IRL customers would need to verify their claim. If C3 uses computer vision, its manufacturing plant would have a frame and timestamp when the customers' duck was evaluated.\n",
    "\n",
    "    Which model would be the best fit for this use case?\n",
    "> Model 1 would be the best fit for this use case because it has the __highest__ *precision*.\n",
    "\n",
    "Model 1 Evaluation\n",
    "- Accuracy               95.00%\n",
    "- Recall                 50.00%\n",
    "- Precision              80.00%\n",
    "- Specificity            98.91%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gives You Paws\n",
    "You are working as a data scientist for Gives You Paws â„¢, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n",
    "At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II).\n",
    "\n",
    "Several models have already been developed with the data, and you can find their results here.\n",
    "\n",
    "Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paws = pd.read_csv('./gives_you_paws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paws['baseline'] = df_paws.actual.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paws = df_paws[['baseline', 'model1', 'model2', 'model3', 'model4', 'actual']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_matrix = pd.crosstab(df_paws.baseline, df_paws.actual) # baseline\n",
    "model1_matrix = pd.crosstab(df_paws.model1, df_paws.actual)  # model 1\n",
    "model2_matrix = pd.crosstab(df_paws.model2, df_paws.actual)  # model 2\n",
    "model3_matrix = pd.crosstab(df_paws.model3, df_paws.actual)  # model 3\n",
    "model4_matrix = pd.crosstab(df_paws.model4, df_paws.actual)  # model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome_matrix(df_paws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives, true_negatives = baseline_matrix.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = round(true_negatives / (true_positives + true_negatives), 2)\n",
    "print(f\"Accuracy {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics(model1_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics(model2_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics(model3_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics(model4_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?\n",
    "> __Model 1 and Model 4__ _outperform_ the baseline model.\n",
    "\n",
    "> __Model 2 and Model 3__ _underperform_ the baseline model.\n",
    "\n",
    "    Suppose you are working on a team that solely deals with dog pictures.\n",
    "    1. Which of these models would you recommend for Phase I?\n",
    "> __Setup the Business Problem__:\n",
    "> C3 provides a `...a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee)`. The company earns its money by providing only cat photos, only dog photos, or cat and dog photos.\n",
    "> 1. They have a tiered system: cat/dog or cat and dog (with an additional charge).\n",
    "\n",
    "> 2. C3 needs to show their users the correct type(s) of animal(s). Otherwise, they are providing a poor service (e.g. a customer wants to see cute cats but sees a bunch of dog photos) and losing potential profit (showing a customer cat AND dog photos for FREE).\n",
    "\n",
    "> __Answer__: I would recommend using __Model 1__ for Phase I because it has the highest __recall__ score. __Recall__ captures all pictures that are actually dogs = True Positives + False Negatives. C3 can see _all_ of its dog photos.\n",
    "> - True Positives = The model correctly classified a dog photo as dog photo.\n",
    "> - False Negatives = The model incorrectly classified a dog photo as a cat photo.\n",
    "\n",
    "    2. For Phase II?\n",
    "> In Phase II I would recommend to use a different metric. __Precision__. With precision, C3's \"Team Ruff\" can find the model that reduces False Positives. False Positives occur when the model predicts a dog photo but it's actually a cat photo.\n",
    "    \n",
    "    Suppose you are working on a team that solely deals with cat pictures.\n",
    "    1. Which of these models would you recommend for Phase I?\n",
    "    2. For Phase II?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem.\n",
    "\n",
    "    sklearn.metrics.accuracy_score\n",
    "    sklearn.metrics.precision_score\n",
    "    sklearn.metrics.recall_score\n",
    "    sklearn.metrics.classification_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
