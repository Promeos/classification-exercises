{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model Evaluation\n",
    "\n",
    "Common ways of evaluating a __classification__ model's performance.\n",
    "> A model is an algorithm/classifier that is fit to the training set.\n",
    " https://docs.aws.amazon.com/machine-learning/latest/dg/training-ml-models.html\n",
    " \n",
    "1. __Confusion matrix__: is a cross-tabulation of a model's predictions against the actual outcome.\n",
    "- A confusion matrix describes the performance of a classification model.\n",
    "https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model predicts whether or not someone like coffee.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual prediction\n",
       "0     coffee  no coffee\n",
       "1  no coffee  no coffee\n",
       "2  no coffee     coffee\n",
       "3     coffee     coffee\n",
       "4     coffee     coffee\n",
       "5     coffee     coffee\n",
       "6  no coffee  no coffee\n",
       "7     coffee  no coffee"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a simplified version on model evaluation to understand the\n",
    "# fundamentals tools to evaluate the models.\n",
    "df = pd.DataFrame({\n",
    "    'actual': ['coffee', 'no coffee', 'no coffee', 'coffee',\n",
    "               'coffee', 'coffee', 'no coffee', 'coffee'],\n",
    "    'prediction': ['no coffee', 'no coffee', 'coffee',\n",
    "                   'coffee', 'coffee', 'coffee', 'no coffee',\n",
    "                   'no coffee'],\n",
    "})\n",
    "print(\"Our model predicts whether or not someone like coffee.\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at pd.crosstab docs to understand kwargs.\n",
    "# pd.crosstab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>coffee</th>\n",
       "      <th>no coffee</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coffee</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no coffee</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual      coffee  no coffee  total\n",
       "prediction                          \n",
       "coffee           3          1      4\n",
       "no coffee        2          2      4\n",
       "total            5          3      8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a confusion matrix\n",
    "pd.crosstab(df.prediction,\n",
    "            df.actual,\n",
    "            margins=True,\n",
    "            margins_name='total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The function accepts actual outcome, predicted outcome.\n",
    "# Actual values first, predicted values second.\n",
    "confusion_M = confusion_matrix(df.actual, df.prediction)\n",
    "confusion_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Working through this simple example, I understand the contents and layout of a confusion matrix!\n",
    "\n",
    "|Confusion Matrix|Outcome|Prediction|Actual|# of People|\n",
    "|:---|:---|:---|:---|:---|\n",
    "|Top Left|True Positive|coffee|coffee|3|\n",
    "|Bottom Right|True Negative|no coffee|no coffee|2|\n",
    "|Top Right|False Positive/Type I Error|coffee|no coffee|1|\n",
    "|Bottom Left|False Negative/Type II Error|no coffee|coffee|2|\n",
    "\n",
    "\n",
    "|Outcome|English|IRL outcome if put into production|\n",
    "|:---|:---|:---|\n",
    "|True Positive|Jarvis predicts a person likes coffee and they do like coffee.|Customer gets coffee. OK.|\n",
    "|True Negative|Jarvis predicts a person does not like coffee and they do not like coffee.|Customer does not get coffee. OK.|\n",
    "|False Positive|Jarvis predicts a person likes coffee and they do not like coffee.|Customer gets coffee they didn't ask for. Awkward...|\n",
    "|False Negative|Jarvis predicts a person does not like coffee and they do like coffee.|Customer doesn't get coffee they wanted. Karen transforms into Godzilla.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "__DummyClassifier__ is a classifier that makes predictions using simple rules. This classifier is useful as a simple baseline to compare with other (real) classifiers.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "Do not use it for real problems.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(constant=None, random_state=None, strategy='most_frequent')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dummy classifier with the strategy as 'most _frequent'.\n",
    "# It predicts the most frequent ACTUAL class/grouping.\n",
    "baseline_classifier = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# Fit the dummy classifier with predictions and actual outcomes.\n",
    "baseline_classifier.fit(df.prediction, df.actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['coffee', 'coffee', 'coffee', 'coffee', 'coffee', 'coffee',\n",
       "       'coffee', 'coffee'], dtype='<U6')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dummy classifier pokemon evolves into its final form, Dummy Model.\n",
    "# The model predicts the most frequent 'prediction'\n",
    "# Meaning if df.prediction has 5 'coffee' and 3 'no coffee'\n",
    "# The classifier will predict that eveyone likes coffee. If this\n",
    "# model was used in a ml product it would give everyone coffee.\n",
    "# Oprah would be proud.\n",
    "baseline_classifier.predict(df.prediction) # EVERYONE LIKE COFFEE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline model's accuracy is 62.50%\n"
     ]
    }
   ],
   "source": [
    "# But, IRL it only gets the prediction right 5/8 times or 62.5%\n",
    "# The score returned is the models accuracy.\n",
    "accuracy = baseline_classifier.score(df.prediction, df.actual)\n",
    "print(f\"The baseline model's accuracy is {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model Evaluation Metrics\n",
    "\n",
    "https://www.ritchieng.com/machine-learning-evaluate-classification-model/\n",
    "\n",
    "### 1. Classification Accuracy\n",
    "Of the _total_ outcomes, how many times does the classification model make __correct__ predictions?\n",
    "\n",
    "The accuracy formula is derived as:\n",
    "\n",
    "|Correct Predictions| = |True Positives + True Negatives|\n",
    "|:---|:---|:---|\n",
    "|Total Number of Predictions| = |True Positives + True Negatives + False Positives + False Negatives|\n",
    "\n",
    "\n",
    "Using the accuracy evaluation metric: The baseline classification model has an accuracy of 5/8 or 62.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline model's accuracy is 62.50%\n"
     ]
    }
   ],
   "source": [
    "# Longhand method\n",
    "confusion_M.ravel() # unravels matrix column wise\n",
    "tp, fn, fp, tn = confusion_M.ravel()\n",
    "\n",
    "# Use the accuracy formula to above to calculate model accuracy\n",
    "accuracy = (tp + tn )/sum([tp, tn, fp, fn])\n",
    "print(f\"The baseline model's accuracy is {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classification Error\n",
    "> Also known as the _misclassification rate_\n",
    "\n",
    "Of the total outcomes, how many times does the classification model make __incorrect__ predictions?\n",
    "\n",
    "The error formula is derived as:\n",
    "\n",
    "|Incorrect Predictions| = |False Positives + False Negatives|\n",
    "|:---|:---|:---|\n",
    "|Total Number of Predictions| = |True Positives + True Negatives + False Positives + False Negatives|\n",
    "\n",
    "The misclassification rate is also calculated as:\n",
    "\n",
    "Classification Error = 1 - accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline model's error is 37.50%\n",
      "Error rate can be calculated as: 1 - accuracy = 37.50%\n"
     ]
    }
   ],
   "source": [
    "error = (fn + fp)/sum([tp,fp,tn,fn])\n",
    "print(f\"The baseline model's error is {error:.2%}\")\n",
    "\n",
    "classification_error = 1 - accuracy\n",
    "print(f\"Error rate can be calculated as: 1 - accuracy = {classification_error:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sensitivity\n",
    "> Also known as __Recall__ or _True Positive Rate_, or __Precision__.\n",
    "\n",
    "> Sensitivity, Precision, or Recall is a metric that should be maximized.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?highlight=recall#sklearn.metrics.recall_score\n",
    "\n",
    "Of the Positive predictions, how many times are they __True Positives__?\n",
    "\n",
    "The sensitivity formula is derived as:\n",
    "\n",
    "|Correct TP Predictions| = |True Positives|\n",
    "|:---|:---|:---|\n",
    "|Total Correct Predictions| = |True Positives + False Negatives|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline model's recall is 60.00%\n",
      "The baseline model's sensitivity is 60.00%\n"
     ]
    }
   ],
   "source": [
    "recall = tp / (tp + tn)\n",
    "print(f\"The baseline model's recall is {recall:.2%}\")\n",
    "\n",
    "sensitivity = sklearn.metrics.recall_score(df.actual, df.prediction, average=\"binary\", pos_label=\"coffee\")\n",
    "print(f\"The baseline model's sensitivity is {sensitivity:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Specificity\n",
    "Of the outcomes that are negative, how often are they __True Negatives__?\n",
    "> Specificity is a metric that should be maximized.\n",
    "\n",
    "The specificity formula is derived as:\n",
    "\n",
    "|Correct TN Predictions| = |True Negatives|\n",
    "|:---|:---|:---|\n",
    "|All Negative Predictions| = |True Negatives + False Negatives|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline model's specificity is 50.00%\n"
     ]
    }
   ],
   "source": [
    "specificity = tn / (tn + fn)\n",
    "print(f\"The baseline model's specificity is {specificity:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      coffee       0.75      0.60      0.67         5\n",
      "   no coffee       0.50      0.67      0.57         3\n",
      "\n",
      "    accuracy                           0.62         8\n",
      "   macro avg       0.62      0.63      0.62         8\n",
      "weighted avg       0.66      0.62      0.63         8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(df.actual, df.prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
