{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600266969848",
   "display_name": "Python 3.8.1 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Classification using Logisitic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from acquire import get_titanic_data\n",
    "from prepare import prep_titanic    \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_titanic = prep_titanic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   passenger_id  survived  pclass     sex   age  sibsp  parch     fare  \\\n0             0         0       3    male  22.0      1      0   7.2500   \n1             1         1       1  female  38.0      1      0  71.2833   \n2             2         1       3  female  26.0      0      0   7.9250   \n3             3         1       1  female  35.0      1      0  53.1000   \n4             4         0       3    male  35.0      0      0   8.0500   \n\n  embarked  class  embark_town  alone  Queenstown  Southampton  Second  Third  \\\n0        S  Third  Southampton      0           0            1       0      1   \n1        C  First    Cherbourg      0           0            0       0      0   \n2        S  Third  Southampton      1           0            1       0      1   \n3        S  First  Southampton      0           0            1       0      0   \n4        S  Third  Southampton      1           0            1       0      1   \n\n   male  \n0     1  \n1     0  \n2     0  \n3     0  \n4     1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>passenger_id</th>\n      <th>survived</th>\n      <th>pclass</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>embarked</th>\n      <th>class</th>\n      <th>embark_town</th>\n      <th>alone</th>\n      <th>Queenstown</th>\n      <th>Southampton</th>\n      <th>Second</th>\n      <th>Third</th>\n      <th>male</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>Southampton</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>C</td>\n      <td>First</td>\n      <td>Cherbourg</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>Southampton</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>S</td>\n      <td>First</td>\n      <td>Southampton</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>Southampton</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.dropna(how='any', subset=['age'], inplace=True)"
   ]
  },
  {
   "source": [
    "# Test Models\n",
    "## _Model 1_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train:  (399, 5) , validate:  (172, 5) , test:  (143, 5)\ntrain:  (399,) , validate:  (172,) , test:  (143,)\n"
    }
   ],
   "source": [
    "# X = df_titanic.loc[:, 'fare':'class_Third']\n",
    "X = df_titanic[['pclass','age','fare','sibsp','parch']]\n",
    "y = df_titanic['survived']\n",
    "\n",
    "X_train_validate, X_test, y_train_validate, y_test = train_test_split(X,\n",
    "                                                                      y,\n",
    "                                                                      test_size=.2,\n",
    "                                                                      random_state=123\n",
    "                                                                      )\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train_validate,\n",
    "                                                            y_train_validate,\n",
    "                                                            test_size=.3,\n",
    "                                                            random_state=123\n",
    "                                                            )\n",
    "\n",
    "\n",
    "print(\"train: \", X_train.shape, \", validate: \", X_validate.shape, \", test: \", X_test.shape)\n",
    "print(\"train: \", y_train.shape, \", validate: \", y_validate.shape, \", test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_model_1 = LogisticRegression(C=1,\n",
    "                                   class_weight={0:1, 1:99},\n",
    "                                   random_state=123,\n",
    "                                   intercept_scaling=1,\n",
    "                                   solver='lbfgs'\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression(C=1, class_weight={0: 1, 1: 99}, random_state=123)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "logit_test_model_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Coefficient: \n [[-1.16035326 -0.03110108  0.00399115 -0.45129797  0.52083144]]\nIntercept: \n [7.65390045]\n"
    }
   ],
   "source": [
    "print('Coefficient: \\n', logit_test_model_1.coef_)\n",
    "print('Intercept: \\n', logit_test_model_1.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n[[5.38032613e-03 9.94619674e-01]\n [9.95313098e-03 9.90046869e-01]\n [1.98570853e-03 9.98014291e-01]\n [1.01714994e-02 9.89828501e-01]\n [3.88457002e-02 9.61154300e-01]\n [2.78936731e-02 9.72106327e-01]\n [3.00998986e-03 9.96990010e-01]\n [3.51375443e-02 9.64862456e-01]\n [5.79521663e-03 9.94204783e-01]\n [1.23720462e-02 9.87627954e-01]\n [2.96226509e-02 9.70377349e-01]\n [3.12745701e-03 9.96872543e-01]\n [6.81894255e-02 9.31810574e-01]\n [3.00444469e-03 9.96995555e-01]\n [2.88169064e-02 9.71183094e-01]\n [3.87545306e-02 9.61245469e-01]\n [4.67502131e-02 9.53249787e-01]\n [9.28750855e-03 9.90712491e-01]\n [3.94670207e-02 9.60532979e-01]\n [2.68650743e-02 9.73134926e-01]\n [1.00878631e-02 9.89912137e-01]\n [2.88169064e-02 9.71183094e-01]\n [4.90928188e-03 9.95090718e-01]\n [1.18800283e-02 9.88119972e-01]\n [6.84042663e-02 9.31595734e-01]\n [3.24256504e-02 9.67574350e-01]\n [5.27138600e-03 9.94728614e-01]\n [3.24422438e-02 9.67557756e-01]\n [2.55817478e-02 9.74418252e-01]\n [4.45208722e-02 9.55479128e-01]\n [7.96059225e-03 9.92039408e-01]\n [5.31323297e-04 9.99468677e-01]\n [5.48649739e-03 9.94513503e-01]\n [5.02350078e-02 9.49764992e-01]\n [1.16346048e-02 9.88365395e-01]\n [9.57808869e-03 9.90421911e-01]\n [6.03912199e-03 9.93960878e-01]\n [3.84524531e-02 9.61547547e-01]\n [4.25847962e-02 9.57415204e-01]\n [1.96800534e-02 9.80319947e-01]\n [4.91233474e-03 9.95087665e-01]\n [3.24444385e-02 9.67555562e-01]\n [5.57680191e-02 9.44231981e-01]\n [4.00013607e-02 9.59998639e-01]\n [9.57808869e-03 9.90421911e-01]\n [1.12450520e-02 9.88754948e-01]\n [8.51869362e-03 9.91481306e-01]\n [1.38579527e-02 9.86142047e-01]\n [8.57386395e-02 9.14261361e-01]\n [2.96307746e-02 9.70369225e-01]\n [3.58204587e-03 9.96417954e-01]\n [7.96632418e-03 9.92033676e-01]\n [7.06329366e-03 9.92936706e-01]\n [4.07267308e-03 9.95927327e-01]\n [5.15526248e-02 9.48447375e-01]\n [9.40831524e-03 9.90591685e-01]\n [9.97908533e-03 9.90020915e-01]\n [4.24345301e-02 9.57565470e-01]\n [2.38360021e-02 9.76163998e-01]\n [1.73494052e-02 9.82650595e-01]\n [7.79812510e-03 9.92201875e-01]\n [3.21990594e-02 9.67800941e-01]\n [3.53588462e-02 9.64641154e-01]\n [4.49469141e-02 9.55053086e-01]\n [7.70728796e-03 9.92292712e-01]\n [7.30728341e-04 9.99269272e-01]\n [8.61575939e-03 9.91384241e-01]\n [2.68902003e-02 9.73109800e-01]\n [3.88010200e-02 9.61198980e-01]\n [7.61257061e-02 9.23874294e-01]\n [3.65465612e-02 9.63453439e-01]\n [2.71283750e-02 9.72871625e-01]\n [3.14821899e-02 9.68517810e-01]\n [9.29098213e-03 9.90709018e-01]\n [6.79627758e-04 9.99320372e-01]\n [4.02589344e-03 9.95974107e-01]\n [2.45206913e-02 9.75479309e-01]\n [1.24403239e-02 9.87559676e-01]\n [1.66454380e-02 9.83354562e-01]\n [2.14563013e-02 9.78543699e-01]\n [2.34601400e-02 9.76539860e-01]\n [1.36267129e-02 9.86373287e-01]\n [8.62066647e-03 9.91379334e-01]\n [9.18905028e-03 9.90810950e-01]\n [7.72426042e-03 9.92275740e-01]\n [6.85524987e-03 9.93144750e-01]\n [9.23785410e-03 9.90762146e-01]\n [5.38963811e-02 9.46103619e-01]\n [7.47914429e-02 9.25208557e-01]\n [2.83748582e-03 9.97162514e-01]\n [2.68541096e-02 9.73145890e-01]\n [1.78750237e-02 9.82124976e-01]\n [1.90395378e-03 9.98096046e-01]\n [1.53313648e-02 9.84668635e-01]\n [7.58533455e-03 9.92414665e-01]\n [5.60762704e-03 9.94392373e-01]\n [1.08331874e-02 9.89166813e-01]\n [8.87097195e-04 9.99112903e-01]\n [7.24761069e-02 9.27523893e-01]\n [2.78063300e-02 9.72193670e-01]\n [4.90794193e-03 9.95092058e-01]\n [1.58876090e-03 9.98411239e-01]\n [8.21000093e-03 9.91789999e-01]\n [6.80131849e-02 9.31986815e-01]\n [3.90309205e-03 9.96096908e-01]\n [3.50495916e-02 9.64950408e-01]\n [1.30266962e-02 9.86973304e-01]\n [5.28642662e-03 9.94713573e-01]\n [4.65687638e-02 9.53431236e-01]\n [9.28750855e-03 9.90712491e-01]\n [3.70980735e-02 9.62901927e-01]\n [7.58176758e-03 9.92418232e-01]\n [3.42265911e-02 9.65773409e-01]\n [1.23069761e-03 9.98769302e-01]\n [3.52719352e-02 9.64728065e-01]\n [2.43085695e-02 9.75691430e-01]\n [7.10838462e-03 9.92891615e-01]\n [1.75340038e-02 9.82465996e-01]\n [3.09785118e-02 9.69021488e-01]\n [3.59777343e-03 9.96402227e-01]\n [3.05150919e-02 9.69484908e-01]\n [3.66626771e-02 9.63337323e-01]\n [2.62531027e-02 9.73746897e-01]\n [6.58396681e-03 9.93416033e-01]\n [6.69365574e-03 9.93306344e-01]\n [4.81229493e-02 9.51877051e-01]\n [5.45384211e-02 9.45461579e-01]\n [2.44470952e-02 9.75552905e-01]\n [2.60382449e-03 9.97396176e-01]\n [2.24817119e-03 9.97751829e-01]\n [5.47986578e-02 9.45201342e-01]\n [1.61929585e-02 9.83807042e-01]\n [2.96657035e-02 9.70334297e-01]\n [2.54527305e-02 9.74547269e-01]\n [2.54799697e-02 9.74520030e-01]\n [3.76005832e-02 9.62399417e-01]\n [3.34195052e-02 9.66580495e-01]\n [2.48200306e-03 9.97517997e-01]\n [8.08358337e-02 9.19164166e-01]\n [3.25492226e-02 9.67450777e-01]\n [8.00801024e-03 9.91991990e-01]\n [2.63685603e-02 9.73631440e-01]\n [4.15687708e-03 9.95843123e-01]\n [3.01214705e-02 9.69878530e-01]\n [3.43167519e-03 9.96568325e-01]\n [4.56784389e-02 9.54321561e-01]\n [2.47816179e-02 9.75218382e-01]\n [4.56728699e-03 9.95432713e-01]\n [1.15500042e-02 9.88449996e-01]\n [2.40840296e-02 9.75915970e-01]\n [3.38358639e-02 9.66164136e-01]\n [7.74933012e-03 9.92250670e-01]\n [4.06222292e-02 9.59377771e-01]\n [1.52148939e-02 9.84785106e-01]\n [9.03235120e-03 9.90967649e-01]\n [6.59942174e-02 9.34005783e-01]\n [5.00912201e-02 9.49908780e-01]\n [6.26948526e-03 9.93730515e-01]\n [5.20958200e-02 9.47904180e-01]\n [2.38018565e-02 9.76198143e-01]\n [2.12478040e-02 9.78752196e-01]\n [5.31351320e-02 9.46864868e-01]\n [2.50921588e-03 9.97490784e-01]\n [1.74256013e-03 9.98257440e-01]\n [5.80299171e-02 9.41970083e-01]\n [4.03009648e-03 9.95969904e-01]\n [2.12561222e-03 9.97874388e-01]\n [3.97562158e-02 9.60243784e-01]\n [5.03820025e-03 9.94961800e-01]\n [2.54799697e-02 9.74520030e-01]\n [4.11886048e-02 9.58811395e-01]\n [6.78493239e-02 9.32150676e-01]\n [3.57218965e-03 9.96427810e-01]\n [9.51996641e-02 9.04800336e-01]\n [3.06333872e-02 9.69366613e-01]\n [5.23155193e-02 9.47684481e-01]\n [1.01336637e-02 9.89866336e-01]\n [7.47724831e-03 9.92522752e-01]\n [1.08481666e-02 9.89151833e-01]\n [1.99920283e-02 9.80007972e-01]\n [9.87766962e-03 9.90122330e-01]\n [3.25087519e-02 9.67491248e-01]\n [2.98119388e-03 9.97018806e-01]\n [2.11461211e-02 9.78853879e-01]\n [1.42854686e-03 9.98571453e-01]\n [1.05256201e-02 9.89474380e-01]\n [3.77009983e-02 9.62299002e-01]\n [4.14342743e-02 9.58565726e-01]\n [4.12248822e-03 9.95877512e-01]\n [1.05256201e-02 9.89474380e-01]\n [2.05905975e-03 9.97940940e-01]\n [3.24100016e-02 9.67589998e-01]\n [8.08135990e-04 9.99191864e-01]\n [4.93769644e-02 9.50623036e-01]\n [1.02876221e-02 9.89712378e-01]\n [3.16465129e-02 9.68353487e-01]\n [5.14936391e-03 9.94850636e-01]\n [1.97678499e-02 9.80232150e-01]\n [6.60463438e-03 9.93395366e-01]\n [6.16188443e-04 9.99383812e-01]\n [5.22270160e-03 9.94777298e-01]\n [4.47794130e-02 9.55220587e-01]\n [7.05537749e-04 9.99294462e-01]\n [3.76859133e-02 9.62314087e-01]\n [4.96386984e-03 9.95036130e-01]\n [3.34324001e-02 9.66567600e-01]\n [4.78100593e-02 9.52189941e-01]\n [2.70150238e-02 9.72984976e-01]\n [2.78860119e-02 9.72113988e-01]\n [5.81659668e-04 9.99418340e-01]\n [3.88239795e-02 9.61176021e-01]\n [3.46955870e-03 9.96530441e-01]\n [1.46560316e-02 9.85343968e-01]\n [2.13404895e-02 9.78659511e-01]\n [1.47788712e-03 9.98522113e-01]\n [1.51995992e-02 9.84800401e-01]\n [6.26355702e-03 9.93736443e-01]\n [6.60622482e-03 9.93393775e-01]\n [6.35326895e-03 9.93646731e-01]\n [3.92400937e-02 9.60759906e-01]\n [4.57219541e-02 9.54278046e-01]\n [3.42508841e-03 9.96574912e-01]\n [3.02104541e-03 9.96978955e-01]\n [1.22002409e-02 9.87799759e-01]\n [4.86166674e-03 9.95138333e-01]\n [1.39880902e-02 9.86011910e-01]\n [2.79441360e-03 9.97205586e-01]\n [1.97295213e-02 9.80270479e-01]\n [2.61548328e-02 9.73845167e-01]\n [3.05451205e-02 9.69454879e-01]\n [2.62356817e-02 9.73764318e-01]\n [4.30229524e-03 9.95697705e-01]\n [3.39505134e-03 9.96604949e-01]\n [1.92987203e-02 9.80701280e-01]\n [1.27621654e-02 9.87237835e-01]\n [2.78310698e-02 9.72168930e-01]\n [1.38511365e-02 9.86148863e-01]\n [1.60168706e-03 9.98398313e-01]\n [2.14563013e-02 9.78543699e-01]\n [1.61755523e-02 9.83824448e-01]\n [4.93914749e-03 9.95060853e-01]\n [1.58241970e-03 9.98417580e-01]\n [3.10926870e-02 9.68907313e-01]\n [3.03443493e-02 9.69655651e-01]\n [9.87766962e-03 9.90122330e-01]\n [2.47355800e-03 9.97526442e-01]\n [8.81909043e-03 9.91180910e-01]\n [4.85659863e-02 9.51434014e-01]\n [8.68558475e-03 9.91314415e-01]\n [1.17165594e-03 9.98828344e-01]\n [4.87717169e-02 9.51228283e-01]\n [5.37774759e-02 9.46222524e-01]\n [6.20618699e-03 9.93793813e-01]\n [2.94996571e-03 9.97050034e-01]\n [3.66733774e-03 9.96332662e-01]\n [2.62514090e-02 9.73748591e-01]\n [5.12171516e-03 9.94878285e-01]\n [2.88196990e-02 9.71180301e-01]\n [4.47268442e-03 9.95527316e-01]\n [2.87583227e-02 9.71241677e-01]\n [9.10757898e-02 9.08924210e-01]\n [2.61284375e-03 9.97387156e-01]\n [3.05805964e-02 9.69419404e-01]\n [5.81225543e-04 9.99418774e-01]\n [2.71472576e-02 9.72852742e-01]\n [1.82078661e-02 9.81792134e-01]\n [5.53822097e-02 9.44617790e-01]\n [3.76973785e-02 9.62302622e-01]\n [1.52292109e-02 9.84770789e-01]\n [1.65654073e-02 9.83434593e-01]\n [2.79022455e-02 9.72097754e-01]\n [2.95428700e-03 9.97045713e-01]\n [3.64605829e-02 9.63539417e-01]\n [1.08331874e-02 9.89166813e-01]\n [1.01360180e-01 8.98639820e-01]\n [3.38759943e-03 9.96612401e-01]\n [3.71592938e-02 9.62840706e-01]\n [3.71408685e-02 9.62859132e-01]\n [3.54671101e-02 9.64532890e-01]\n [1.48976578e-02 9.85102342e-01]\n [1.58910441e-02 9.84108956e-01]\n [2.70551783e-02 9.72944822e-01]\n [5.06357969e-03 9.94936420e-01]\n [2.61548328e-02 9.73845167e-01]\n [4.18833259e-02 9.58116674e-01]\n [3.56250310e-04 9.99643750e-01]\n [7.72331722e-03 9.92276683e-01]\n [2.60203029e-02 9.73979697e-01]\n [2.82357949e-03 9.97176421e-01]\n [3.38716796e-02 9.66128320e-01]\n [1.31556069e-02 9.86844393e-01]\n [3.06225210e-02 9.69377479e-01]\n [1.32193708e-02 9.86780629e-01]\n [6.97459805e-03 9.93025402e-01]\n [3.88196308e-02 9.61180369e-01]\n [3.38064443e-03 9.96619356e-01]\n [5.80538182e-02 9.41946182e-01]\n [2.79157806e-02 9.72084219e-01]\n [2.57313057e-03 9.97426869e-01]\n [1.05049340e-02 9.89495066e-01]\n [1.30266962e-02 9.86973304e-01]\n [3.23350481e-03 9.96766495e-01]\n [2.97408848e-03 9.97025912e-01]\n [4.27934936e-02 9.57206506e-01]\n [9.25847956e-03 9.90741520e-01]\n [2.41483211e-03 9.97585168e-01]\n [2.32399052e-02 9.76760095e-01]\n [3.10299589e-02 9.68970041e-01]\n [1.24510574e-02 9.87548943e-01]\n [3.35640652e-02 9.66435935e-01]\n [1.96398112e-03 9.98036019e-01]\n [8.74851027e-03 9.91251490e-01]\n [3.46141302e-02 9.65385870e-01]\n [3.15461421e-02 9.68453858e-01]\n [4.25294826e-02 9.57470517e-01]\n [3.14862425e-02 9.68513757e-01]\n [1.51406058e-03 9.98485939e-01]\n [3.83838440e-02 9.61616156e-01]\n [3.09290115e-03 9.96907099e-01]\n [6.51334088e-02 9.34866591e-01]\n [5.14671739e-02 9.48532826e-01]\n [5.23973996e-03 9.94760260e-01]\n [2.79067601e-02 9.72093240e-01]\n [6.23077378e-02 9.37692262e-01]\n [6.87188958e-03 9.93128110e-01]\n [5.97069700e-03 9.94029303e-01]\n [2.41483211e-03 9.97585168e-01]\n [6.03466082e-03 9.93965339e-01]\n [5.52904482e-03 9.94470955e-01]\n [1.13910104e-01 8.86089896e-01]\n [1.15204243e-02 9.88479576e-01]\n [5.53822097e-02 9.44617790e-01]\n [2.69189353e-02 9.73081065e-01]\n [3.04207744e-02 9.69579226e-01]\n [4.11886048e-02 9.58811395e-01]\n [5.81225543e-04 9.99418774e-01]\n [2.39545200e-02 9.76045480e-01]\n [3.19927439e-02 9.68007256e-01]\n [2.79049520e-02 9.72095048e-01]\n [3.44443835e-02 9.65555616e-01]\n [7.50559918e-03 9.92494401e-01]\n [4.25970022e-02 9.57402998e-01]\n [2.19374275e-02 9.78062573e-01]\n [2.25366404e-02 9.77463360e-01]\n [1.38511365e-02 9.86148863e-01]\n [3.42265911e-02 9.65773409e-01]\n [1.66454380e-02 9.83354562e-01]\n [8.96705910e-02 9.10329409e-01]\n [3.35144566e-02 9.66485543e-01]\n [1.11737741e-02 9.88826226e-01]\n [3.34689496e-03 9.96653105e-01]\n [2.51697891e-03 9.97483021e-01]\n [3.54881696e-02 9.64511830e-01]\n [5.00084100e-02 9.49991590e-01]\n [1.55494998e-02 9.84450500e-01]\n [2.85399996e-02 9.71460000e-01]\n [1.19004285e-02 9.88099572e-01]\n [5.70963694e-02 9.42903631e-01]\n [1.47074971e-03 9.98529250e-01]\n [2.87611098e-02 9.71238890e-01]\n [4.85659863e-02 9.51434014e-01]\n [3.45917973e-03 9.96540820e-01]\n [6.70601957e-03 9.93293980e-01]\n [3.88196308e-02 9.61180369e-01]\n [3.76756669e-02 9.62324333e-01]\n [5.71576092e-02 9.42842391e-01]\n [1.67770402e-02 9.83222960e-01]\n [2.52759530e-02 9.74724047e-01]\n [1.02587345e-02 9.89741265e-01]\n [2.74692743e-02 9.72530726e-01]\n [9.28750855e-03 9.90712491e-01]\n [4.35964272e-04 9.99564036e-01]\n [4.78100593e-02 9.52189941e-01]\n [3.98840177e-02 9.60115982e-01]\n [3.98852862e-02 9.60114714e-01]\n [1.22507182e-02 9.87749282e-01]\n [2.05118582e-02 9.79488142e-01]\n [1.26172548e-03 9.98738275e-01]\n [9.67320686e-03 9.90326793e-01]\n [3.15705379e-02 9.68429462e-01]\n [6.46852534e-02 9.35314747e-01]\n [4.87093054e-04 9.99512907e-01]\n [5.66606334e-03 9.94333937e-01]\n [9.57808869e-03 9.90421911e-01]\n [2.87611098e-02 9.71238890e-01]\n [3.08634124e-02 9.69136588e-01]\n [1.45927507e-02 9.85407249e-01]\n [1.09406321e-02 9.89059368e-01]\n [4.00013607e-02 9.59998639e-01]\n [2.87276822e-02 9.71272318e-01]\n [9.72949830e-03 9.90270502e-01]\n [2.62637362e-02 9.73736264e-01]\n [2.46337484e-02 9.75366252e-01]\n [1.35655519e-02 9.86434448e-01]\n [9.87766962e-03 9.90122330e-01]\n [9.10187214e-03 9.90898128e-01]\n [7.71588385e-03 9.92284116e-01]\n [2.70626176e-02 9.72937382e-01]\n [7.24659479e-04 9.99275341e-01]]\n"
    }
   ],
   "source": [
    "y_pred = logit_test_model_1.predict(X_train)\n",
    "y_pred_proba = logit_test_model_1.predict_proba(X_train)\n",
    "print(y_pred)\n",
    "print(y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy of Logistic Regression classifier on training set: 0.43\n"
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit_test_model_1.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[  0 228]\n [  0 171]]\n"
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       228\n           1       0.43      1.00      0.60       171\n\n    accuracy                           0.43       399\n   macro avg       0.21      0.50      0.30       399\nweighted avg       0.18      0.43      0.26       399\n\n"
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "source": [
    "## _Model 2_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_model_2 = LogisticRegression(C=.1,\n",
    "class_weight={0:1, 1:99},\n",
    "random_state=123,\n",
    "intercept_scaling=1,\n",
    "solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression(C=0.1, class_weight={0: 1, 1: 99}, random_state=123)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "logit_test_model_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Coefficient: \n [[-1.00329345 -0.02877517  0.00604013 -0.40518742  0.44919365]]\nIntercept: \n [7.14670159]\n"
    }
   ],
   "source": [
    "print('Coefficient: \\n', logit_test_model_2.coef_)\n",
    "print('Intercept: \\n', logit_test_model_2.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy of Logistic Regression classifier on training set: 0.43\n[[  0 228]\n [  0 171]]\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       228\n           1       0.43      1.00      0.60       171\n\n    accuracy                           0.43       399\n   macro avg       0.21      0.50      0.30       399\nweighted avg       0.18      0.43      0.26       399\n\n"
    }
   ],
   "source": [
    "y_pred2 = logit_test_model_2.predict(X_train)\n",
    "y_pred_proba2 = logit_test_model_2.predict_proba(X_train)\n",
    "\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit_test_model_2.score(X_train, y_train)))\n",
    "\n",
    "print(confusion_matrix(y_train, y_pred2))\n",
    "\n",
    "print(classification_report(y_train, y_pred2))"
   ]
  },
  {
   "source": [
    "# Logistic Regression Exercises"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.\n",
    "Start by defining your baseline model. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## 2.\n",
    "Create another model that includes `age` in addition to `fare` and `pclass`. Does this model perform better than your previous one?\n",
    "### Model 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train:  (399, 3) , validate:  (172, 3) , test:  (143, 3)\ntrain:  (399,) , validate:  (172,) , test:  (143,)\n"
    }
   ],
   "source": [
    "X = df_titanic[['pclass', 'age', 'fare']]\n",
    "y = df_titanic['survived']\n",
    "\n",
    "X_train_validate, X_test, y_train_validate, y_test = train_test_split(X,\n",
    "                                                                      y,\n",
    "                                                                      test_size=.2,\n",
    "                                                                      random_state=123\n",
    "                                                                      )\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train_validate,\n",
    "                                                            y_train_validate,\n",
    "                                                            test_size=.3,\n",
    "                                                            random_state=123\n",
    "                                                            )\n",
    "\n",
    "\n",
    "print(\"train: \", X_train.shape, \", validate: \", X_validate.shape, \", test: \", X_test.shape)\n",
    "print(\"train: \", y_train.shape, \", validate: \", y_validate.shape, \", test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(random_state=123, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression(random_state=123)"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Coefficient: \n [[-1.00444388 -0.02941366  0.00518596]]\nIntercept: \n [2.60928736]\nAccuracy of Logistic Regression classifier on training set: 0.70\n[[191  37]\n [ 81  90]]\n              precision    recall  f1-score   support\n\n           0       0.70      0.84      0.76       228\n           1       0.71      0.53      0.60       171\n\n    accuracy                           0.70       399\n   macro avg       0.71      0.68      0.68       399\nweighted avg       0.70      0.70      0.70       399\n\n"
    }
   ],
   "source": [
    "print('Coefficient: \\n', logit.coef_)\n",
    "print('Intercept: \\n', logit.intercept_)\n",
    "\n",
    "y_pred = logit.predict(X_train)\n",
    "y_pred_proba = logit.predict_proba(X_train)\n",
    "\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_train, y_train)))\n",
    "\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "source": [
    "## 3.\n",
    "Include `sex` in your model as well.\n",
    "> Note that you'll need to encode or create a dummy variable of this feature before including it in a model.\n",
    "\n",
    "### Model 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train:  (399, 3) , validate:  (172, 3) , test:  (143, 4)\ntrain:  (399,) , validate:  (172,) , test:  (143,)\n"
    }
   ],
   "source": [
    "X = df_titanic[['pclass', 'age', 'fare', 'male']]\n",
    "y = df_titanic['survived']\n",
    "\n",
    "X_train_validate2, X_test2, y_train_validate2, y_test2 = train_test_split(X,\n",
    "                                                                      y,\n",
    "                                                                      test_size=.2,\n",
    "                                                                      random_state=123\n",
    "                                                                      )\n",
    "\n",
    "X_train2, X_validate2, y_train2, y_validate2 = train_test_split(X_train_validate,\n",
    "                                                            y_train_validate,\n",
    "                                                            test_size=.3,\n",
    "                                                            random_state=123\n",
    "                                                            )\n",
    "\n",
    "\n",
    "print(\"train: \", X_train2.shape, \", validate: \", X_validate2.shape, \", test: \", X_test2.shape)\n",
    "print(\"train: \", y_train2.shape, \", validate: \", y_validate2.shape, \", test: \", y_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_2 = LogisticRegression(random_state=123, intercept_scaling=1, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Coefficient: \n [[-1.00444388 -0.02941366  0.00518596]]\nIntercept: \n [2.60928736]\nAccuracy of Logistic Regression classifier on training set: 0.70\n[[191  37]\n [ 81  90]]\n              precision    recall  f1-score   support\n\n           0       0.70      0.84      0.76       228\n           1       0.71      0.53      0.60       171\n\n    accuracy                           0.70       399\n   macro avg       0.71      0.68      0.68       399\nweighted avg       0.70      0.70      0.70       399\n\n"
    }
   ],
   "source": [
    "logit_2.fit(X_train2, y_train2)\n",
    "\n",
    "print('Coefficient: \\n', logit_2.coef_)\n",
    "print('Intercept: \\n', logit_2.intercept_)\n",
    "\n",
    "y_pred2 = logit_2.predict(X_train2)\n",
    "y_pred_proba2 = logit_2.predict_proba(X_train2)\n",
    "\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit_2.score(X_train2, y_train2)))\n",
    "\n",
    "print(confusion_matrix(y_train2, y_pred2))\n",
    "\n",
    "print(classification_report(y_train2, y_pred2))"
   ]
  },
  {
   "source": [
    "## 4.\n",
    "Try out other combinations of features and models.\n",
    "\n",
    "### Model 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train:  (399, 6) , validate:  (172, 6) , test:  (143, 6)\ntrain:  (399,) , validate:  (172,) , test:  (143,)\n"
    }
   ],
   "source": [
    "X = df_titanic[['pclass', 'age', 'fare', 'male', 'Queenstown', 'Southampton']]\n",
    "y = df_titanic['survived']\n",
    "\n",
    "X_train_validate3, X_test3, y_train_validate3, y_test3 = train_test_split(X,\n",
    "                                                                          y,\n",
    "                                                                          test_size=.2,\n",
    "                                                                          random_state=123\n",
    "                                                                          )\n",
    "\n",
    "X_train3, X_validate3, y_train3, y_validate3 = train_test_split(X_train_validate3,\n",
    "                                                            y_train_validate3,\n",
    "                                                            test_size=.3,\n",
    "                                                            random_state=123\n",
    "                                                            )\n",
    "\n",
    "\n",
    "print(\"train: \", X_train3.shape, \", validate: \", X_validate3.shape, \", test: \", X_test3.shape)\n",
    "print(\"train: \", y_train3.shape, \", validate: \", y_validate3.shape, \", test: \", y_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_3 = LogisticRegression(random_state=123, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Coefficient: \n [[-1.05899184e+00 -2.77741850e-02  2.50423675e-03 -2.53771314e+00\n  -9.11492204e-01 -4.04828933e-01]]\nIntercept: \n [4.71610197]\nAccuracy of Logistic Regression classifier on training set: 0.81\n[[196  32]\n [ 44 127]]\n              precision    recall  f1-score   support\n\n           0       0.82      0.86      0.84       228\n           1       0.80      0.74      0.77       171\n\n    accuracy                           0.81       399\n   macro avg       0.81      0.80      0.80       399\nweighted avg       0.81      0.81      0.81       399\n\n"
    }
   ],
   "source": [
    "logit_3.fit(X_train3, y_train3)\n",
    "\n",
    "print('Coefficient: \\n', logit_3.coef_)\n",
    "print('Intercept: \\n', logit_3.intercept_)\n",
    "\n",
    "y_pred3 = logit_3.predict(X_train3)\n",
    "y_pred_proba3 = logit_3.predict_proba(X_train3)\n",
    "\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit_3.score(X_train3, y_train3)))\n",
    "\n",
    "print(confusion_matrix(y_train3, y_pred3))\n",
    "\n",
    "print(classification_report(y_train3, y_pred3))"
   ]
  },
  {
   "source": [
    "### Model 4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train:  (399, 2) , validate:  (172, 2) , test:  (143, 2)\ntrain:  (399,) , validate:  (172,) , test:  (143,)\n"
    }
   ],
   "source": [
    "X = df_titanic[['fare', 'male']]\n",
    "y = df_titanic['survived']\n",
    "\n",
    "X_train_validate4, X_test4, y_train_validate4, y_test4 = train_test_split(X,\n",
    "                                                                      y,\n",
    "                                                                      test_size=.2,\n",
    "                                                                      random_state=123\n",
    "                                                                      )\n",
    "\n",
    "X_train4, X_validate4, y_train4, y_validate4 = train_test_split(X_train_validate4,\n",
    "                                                            y_train_validate4,\n",
    "                                                            test_size=.3,\n",
    "                                                            random_state=123\n",
    "                                                            )\n",
    "\n",
    "\n",
    "print(\"train: \", X_train4.shape, \", validate: \", X_validate4.shape, \", test: \", X_test4.shape)\n",
    "print(\"train: \", y_train4.shape, \", validate: \", y_validate4.shape, \", test: \", y_test4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_4 = LogisticRegression(random_state=123, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Coefficient: \n [[ 0.01527599 -2.44659372]]\nIntercept: \n [0.74930988]\nAccuracy of Logistic Regression classifier on training set: 0.79\n[[196  32]\n [ 51 120]]\n              precision    recall  f1-score   support\n\n           0       0.79      0.86      0.83       228\n           1       0.79      0.70      0.74       171\n\n    accuracy                           0.79       399\n   macro avg       0.79      0.78      0.78       399\nweighted avg       0.79      0.79      0.79       399\n\n"
    }
   ],
   "source": [
    "logit_4.fit(X_train4, y_train4)\n",
    "\n",
    "print('Coefficient: \\n', logit_4.coef_)\n",
    "print('Intercept: \\n', logit_4.intercept_)\n",
    "\n",
    "y_pred4 = logit_4.predict(X_train4)\n",
    "y_pred_proba4 = logit_4.predict_proba(X_train4)\n",
    "\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit_4.score(X_train4, y_train4)))\n",
    "\n",
    "print(confusion_matrix(y_train4, y_pred4))\n",
    "\n",
    "print(classification_report(y_train4, y_pred4))"
   ]
  },
  {
   "source": [
    "## 5.\n",
    "Use the best 3 models to predict and evaluate on the validate sample."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy Scores\n---------------\nModel 2: 70.43%\nModel 3: 80.95%\nModel 4: 79.20%\n"
    }
   ],
   "source": [
    "print(\"Accuracy Scores\")\n",
    "print('-' * 15)\n",
    "print(f\"Model 2: {logit_2.score(X_train2, y_train2):.2%}\")\n",
    "print(f\"Model 3: {logit_3.score(X_train3, y_train3):.2%}\")\n",
    "print(f\"Model 4: {logit_4.score(X_train4, y_train4):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model 2: solver = lbfgs, c = 1\nAccuracy: 0.727\n[[88 21]\n [26 37]]\n              precision    recall  f1-score   support\n\n           0       0.77      0.81      0.79       109\n           1       0.64      0.59      0.61        63\n\n    accuracy                           0.73       172\n   macro avg       0.70      0.70      0.70       172\nweighted avg       0.72      0.73      0.72       172\n\nModel 3: solver = lbfgs, c = .1\nAccuracy: 0.779\n[[85 24]\n [14 49]]\n              precision    recall  f1-score   support\n\n           0       0.86      0.78      0.82       109\n           1       0.67      0.78      0.72        63\n\n    accuracy                           0.78       172\n   macro avg       0.76      0.78      0.77       172\nweighted avg       0.79      0.78      0.78       172\n\nModel 4: solver = lbfgs, c = .1\nAccuracy: 0.779\n[[89 20]\n [18 45]]\n              precision    recall  f1-score   support\n\n           0       0.83      0.82      0.82       109\n           1       0.69      0.71      0.70        63\n\n    accuracy                           0.78       172\n   macro avg       0.76      0.77      0.76       172\nweighted avg       0.78      0.78      0.78       172\n\n"
    }
   ],
   "source": [
    "y_pred2 = logit_2.predict(X_validate2)\n",
    "y_pred3 = logit_3.predict(X_validate3)\n",
    "y_pred4 = logit_4.predict(X_validate4)\n",
    "\n",
    "print(\"Model 2: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.3f}'.format(logit_2.score(X_validate2, y_validate2)))\n",
    "\n",
    "print(confusion_matrix(y_validate2, y_pred2))\n",
    "\n",
    "print(classification_report(y_validate2, y_pred2))\n",
    "\n",
    "print(\"Model 3: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.3f}'.format(logit_3.score(X_validate3, y_validate3)))\n",
    "\n",
    "print(confusion_matrix(y_validate3, y_pred3))\n",
    "\n",
    "print(classification_report(y_validate3, y_pred3))\n",
    "\n",
    "print(\"Model 4: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.3f}'.format(logit_4.score(X_validate4, y_validate4)))\n",
    "\n",
    "print(confusion_matrix(y_validate4, y_pred4))\n",
    "\n",
    "print(classification_report(y_validate4, y_pred4))\n"
   ]
  },
  {
   "source": [
    "##  6.\n",
    "Choose you best model from the validation performation, and evaluate it on the test dataset. How do the performance metrics compare to validate? to train? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model 4: solver = lbfgs, c = 1\nAccuracy: 0.755\n[[68 19]\n [16 40]]\n              precision    recall  f1-score   support\n\n           0       0.81      0.78      0.80        87\n           1       0.68      0.71      0.70        56\n\n    accuracy                           0.76       143\n   macro avg       0.74      0.75      0.75       143\nweighted avg       0.76      0.76      0.76       143\n\n"
    }
   ],
   "source": [
    "y_pred3 = logit_3.predict(X_test3)\n",
    "\n",
    "print(\"Model 4: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.3f}'.format(logit_3.score(X_test3, y_test3)))\n",
    "\n",
    "print(confusion_matrix(y_test3, y_pred3))\n",
    "\n",
    "print(classification_report(y_test3, y_pred3))"
   ]
  }
 ]
}